{"meta":{"title":"大漠说程序","subtitle":"大漠孤烟直，长河落日圆","description":"KK's personal website for posting software technology","author":"大漠叔叔","url":"https://damoshushu.github.io"},"pages":[{"title":"about","date":"2018-11-14T17:18:52.000Z","updated":"2019-01-12T10:18:25.000Z","comments":true,"path":"about/index.html","permalink":"https://damoshushu.github.io/about/index.html","excerpt":"","text":"大漠叔叔"},{"title":"categories","date":"2018-11-18T19:49:06.000Z","updated":"2019-01-12T10:18:25.000Z","comments":true,"path":"categories/index.html","permalink":"https://damoshushu.github.io/categories/index.html","excerpt":"","text":""},{"title":"All tags","date":"2018-11-18T19:48:59.000Z","updated":"2019-01-12T10:18:25.000Z","comments":true,"path":"tags/index.html","permalink":"https://damoshushu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"[K8S] 如何利用terminationGracePeriodSeconds 优雅地关闭你的服务？","slug":"k8s-terminationGracePeriodSeconds","date":"2019-01-12T17:30:06.000Z","updated":"2019-01-12T10:18:25.000Z","comments":true,"path":"2019/01/12/k8s-terminationGracePeriodSeconds/","link":"","permalink":"https://damoshushu.github.io/2019/01/12/k8s-terminationGracePeriodSeconds/","excerpt":"","text":"前言K8S 是一个管理容器的好框架，可是， 用了K8S就能做到服务在版本升级过程中的零停机了吗？非也，如果不正确的使用或者配置K8S，可能会导致在升级过程中部分用户请求失败。 记得上一篇说了如何 readiness probe 在 K8S 的滚动升级中的重要性。今天继续说说K8S滚动升级中用到的的另一个重要参数: terminationGracePeriodSeconds. 什么是 terminationGracePeriodSeconds解释这个参数之前，先来回忆一下K8S滚动升级的步骤: K8S首先启动新的POD K8S等待新的POD进入Ready状态 K8S创建Endpoint，将新的POD纳入负载均衡 K8S移除与老POD相关的Endpoint，并且将老POD状态设置为Terminating，此时将不会有新的请求到达老POD 同时 K8S 会给老POD发送SIGTERM信号，并且等待 terminationGracePeriodSeconds 这么长的时间。(默认为30秒) 超过terminationGracePeriodSeconds等待时间后， K8S 会强制结束老POD 看到这里，我想大家应该明白了，terminationGracePeriodSeconds 就是K8S给你程序留的最后的缓冲时间，来处理关闭之前的操作。 问题剖析或许你会问，如果不配置或者不处理这个，有什么问题？假象一下下面的场景： K8S会给你30秒的缓冲，假如，你有一个操作需要很长的时间，超过了30秒，那么这个请求是不是就会失败？ 如果你的程序没有处理SIGTERM信号，默认是收到这个信号会终止程序，那么当前Pod里面现存的所有用户请求都会失败 解决问题既然知道可能带来的问题了，那么怎么解决呢？明白这个参数的意义，那就很好解决了。 首先 terminationGracePeriodSeconds 要设置一个合适的值，至少保证所有现存的request能被正确处理并返回 你的程序需要处理SIGTERM信号，并且保证所有事务完成后再关闭程序。参见例子: springboot-graceful-terminator 注意： 如果你的程序不处理SIGTERM信号，JVM会默认直接关闭，所以不能保证所有现存事务被正确处理，这样会导致部分现存请求失败","categories":[{"name":"微服务","slug":"微服务","permalink":"https://damoshushu.github.io/categories/微服务/"}],"tags":[{"name":"云服务","slug":"云服务","permalink":"https://damoshushu.github.io/tags/云服务/"},{"name":"K8S","slug":"K8S","permalink":"https://damoshushu.github.io/tags/K8S/"},{"name":"微服务","slug":"微服务","permalink":"https://damoshushu.github.io/tags/微服务/"},{"name":"RollingUpdate","slug":"RollingUpdate","permalink":"https://damoshushu.github.io/tags/RollingUpdate/"},{"name":"terminationGracePeriodSeconds","slug":"terminationGracePeriodSeconds","permalink":"https://damoshushu.github.io/tags/terminationGracePeriodSeconds/"}]},{"title":"[K8S Rolling Update]说好的Zero-downtime 为什么down了呢？","slug":"K8s-rolling-update-readiness-probe","date":"2018-12-02T15:30:06.000Z","updated":"2019-01-12T10:18:25.000Z","comments":true,"path":"2018/12/02/K8s-rolling-update-readiness-probe/","link":"","permalink":"https://damoshushu.github.io/2018/12/02/K8s-rolling-update-readiness-probe/","excerpt":"","text":"缘起目前在做的一个项目采用Kubernetes(简称K8S)来管理容器，然而，最近发现在做升级部署的时候，有很少一部分请求会失败。这就不符合规矩了啊，按照道理说，使用K8S做Rolling Update，应该会无痛切换，平稳从Version A 升级到 Version B。 来，看图： 解决问题前，先磨叽磨叽，说几句废话，心急的可以跳过。点击直达 Rolling Update Zero-downtime 原理K8S提供了一系列功能来支撑容器集群的自动化部署、扩容以及运维，其中一个功能便是今天要讲到的Rolling Update. 引用官方的一段话 To update a service without an outage, kubectl supports what is called rolling update, which updates one pod at a time, rather than taking down the entire service at the same time. P.S: 目前官方不推荐使用“Replication Controllers”， 转而使用 Deployment，不过这里的原理是一样的。 在K8S集群里面，一个Pod里面的程序访问另一个Pod里面的程序，可以采用Service来实现，每一个Service都可以定义一个Selector来选择一系列Pod，当Service接收到请求后，它会转发给后面的Pod来处理。 K8S 执行 Rolling Update的时候，Service是不会改变的，Rolling Update只针对Replication Controllers。具体步骤如下： K8S 创建一个新的Replication Controllers K8S 创建一个新的Pod K8S 等待Pod进入Ready状态 K8S 把新的Pod加入Service K8S 更改老的Pod状态为Terminating，并把老Pod从Service中移除 K8S 结束老的Pod 其中K8S 等待Pod进入Ready状态是需要Pod提供一个Readiness探针，参见 Configure Liveness and Readiness Probes 看似完美，可是为什么出问题了呢？ 故障分析 第一步，那一定是看日志，没毛病。查看了一系列日志，发现在做音频转码的时候，报错 “connection refused”。 意识到转码服务在某一时刻是不工作的。 第二步，查看转码服务工作原理，转码服务提供两个端口，分别对应于命令通道和数据通道。 第三步，由于无法确认是在哪一个端口被拒绝了，于是启用Debug日志，发现是在连接数据通道的时候出问题了。 第四步，分析，理论上来说只有在Pod Ready后，K8S才会把流量转交给这个Pod。那么什么情况下会在K8S会把流量转交给一个没有Ready的Pod呢？ 第五步，查看Readiness定义, 这个Component提供了两个Port，一个用于控制命令，一个用于数据命令。问题就出在这里了，Readiness probe采用了tcpSocket类型的probe, (详见) 但是这里只能配置一个端口，在某种极端情况下，两个端口的就绪时间可能不一致，导致了K8S探测到component已经就绪，但实际上有一个端口没有就绪。从而导致个别请求失败。 解决方案 内部实现 readiness http api并暴露给K8S 或者 采用exec + command 类型的probe, component内部检测到端口就绪后，写一个ready文件到磁盘。 详见 或者 延长 “initialDelaySeconds” 来等待component 就绪（不推荐） p.s:“initialDelaySeconds” 是指 Pod处于running状态后，K8S等待多少秒开始第一次执行readiness probe。 结论自此，Rolling Update的问题完美解决了。Kubernetes 作为一个容器管理系统，还是很强大的。 成功图：","categories":[{"name":"微服务","slug":"微服务","permalink":"https://damoshushu.github.io/categories/微服务/"}],"tags":[{"name":"云服务","slug":"云服务","permalink":"https://damoshushu.github.io/tags/云服务/"},{"name":"K8S","slug":"K8S","permalink":"https://damoshushu.github.io/tags/K8S/"},{"name":"微服务","slug":"微服务","permalink":"https://damoshushu.github.io/tags/微服务/"},{"name":"RollingUpdate","slug":"RollingUpdate","permalink":"https://damoshushu.github.io/tags/RollingUpdate/"}]},{"title":"使用GitLab Ci 自动部署Hexo到GitHub","slug":"UseGitLabDeployHexo2GitHub","date":"2018-11-18T20:30:06.000Z","updated":"2019-01-12T10:18:25.000Z","comments":true,"path":"2018/11/18/UseGitLabDeployHexo2GitHub/","link":"","permalink":"https://damoshushu.github.io/2018/11/18/UseGitLabDeployHexo2GitHub/","excerpt":"","text":"使用GitLab Ci 自动部署Hexo到GitHub也许这个操作有点傻(sao)，为啥有GitLab Pages了，还要在GitHub 上搭建Hexo呢？好吧，不管那么多了，反正就是可以。 前言GitHub 提供pages 功能，具体参见 What is GitHub Pages?，可以让你搭建自己的网站，当然只能是静态的。但是可以利用第三方博客框架自动生成静态网站。Hexo就是其中一个，至于如何使用，网上搜索一大堆，这里不赘述。 但是Hexo是基于NodeJs的，那么问题来了，每次写完文章，都需要Hexo g编译一次，还要上传编译好的文件到GitHub上面，这就限制了随时随地写文章的冲动。 Gitlab是个好东西，不但提供了Private Repository, 还提供了GitLab CI持续集成部署环境。 基本思路随时随地任意终端写MarkDown文章 –&gt; 到GitLab的“_posts”目录添加文件 –&gt; 自动触发GitLab CI Pipeline –&gt; 自动编译并部署到GitHub 具体实现 首先在GitHub上面创建一个repo，开启Pages 在GitLab上面创建一个Private的repo用来放博客的源代码 在GitLab添加一个SSH Private Key 文件，用于push到GitHub 在GitLab上添加 .gitlab-ci.yml 文件用户自动部署 准备就绪，下面是.gitlab-ci.yml的具体内容：1234567891011121314151617181920212223242526variables: BUILD_IMAGE: node:9.8.0stages: - build############### build job##############build_job: stage: build image: $BUILD_IMAGE cache: paths: - node_modules/ script: - npm install hexo -g - npm install - npm install hexo-deployer-git --save - npm install hexo-wordcount --save - eval $(ssh-agent -s) - chmod 700 cctgrsa - ssh-add cctgrsa - git config --global user.email &quot;damoshushu@cctg2010.com&quot; - git config --global user.name &quot;CCTG2010GitLab&quot; - echo StrictHostKeyChecking no &gt;&gt; /etc/ssh/ssh_config - hexo d -g - echo &quot;Deploy succssed!&quot; 可以直接Clone https://gitlab.com/damoshushu/github-pages-auto-deployer.git 注解一下： cctgrsa 文件是 GitHub 的private ssh key，push操作的时候会用到，由于我们放在GitLab的private 项目里面，所以不用担心被人看到 “echo StrictHostKeyChecking no &gt;&gt; /etc/ssh/ssh_config” 是为了运行git push的时候不弹出是否继续的提示 “npm install hexo-deployer-git” 插件可以直接push到git 结果展示GitLab 私有仓库目录结构如下： Pipeline 自动触发如下： 我的博客地址：大漠叔叔的博客","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://damoshushu.github.io/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://damoshushu.github.io/tags/DevOps/"},{"name":"GitLab-CI","slug":"GitLab-CI","permalink":"https://damoshushu.github.io/tags/GitLab-CI/"}]},{"title":"从源码分析如何优雅的使用 Kafka 生产者","slug":"KafkaConsumer","date":"2018-11-16T15:30:06.000Z","updated":"2019-01-12T10:18:25.000Z","comments":true,"path":"2018/11/16/KafkaConsumer/","link":"","permalink":"https://damoshushu.github.io/2018/11/16/KafkaConsumer/","excerpt":"","text":"转于JCSprout 简单的消息发送在分析之前先看一个简单的消息发送是怎么样的。 以下代码基于 SpringBoot 构建。 首先创建一个 org.apache.kafka.clients.producer.Producer 的 bean。 主要关注 bootstrap.servers，它是必填参数。指的是 Kafka 集群中的 broker 地址，例如 127.0.0.1:9094。 其余几个参数暂时不做讨论，后文会有详细介绍。 接着注入这个 bean 即可调用它的发送函数发送消息。 这里我给某一个 Topic 发送了 10W 条数据，运行程序消息正常发送。 但这仅仅只是做到了消息发送，对消息是否成功送达完全没管，等于是纯异步的方式。 同步那么我想知道消息到底发送成功没有该怎么办呢？ 其实 Producer 的 API 已经帮我们考虑到了，发送之后只需要调用它的 get() 方法即可同步获取发送结果。 发送结果： 这样的发送效率其实是比较低下的，因为每次都需要同步等待消息发送的结果。 异步为此我们应当采取异步的方式发送，其实 send() 方法默认则是异步的，只要不手动调用 get() 方法。 但这样就没法获知发送结果。 所以查看 send() 的 API 可以发现还有一个参数。 1Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; producer, Callback callback); Callback 是一个回调接口，在消息发送完成之后可以回调我们自定义的实现。 执行之后的结果： 同样的也能获取结果，同时发现回调的线程并不是上文同步时的主线程，这样也能证明是异步回调的。 同时回调的时候会传递两个参数： RecordMetadata 和上文一致的消息发送成功后的元数据。 Exception 消息发送过程中的异常信息。 但是这两个参数并不会同时都有数据，只有发送失败才会有异常信息，同时发送元数据为空。 所以正确的写法应当是： 至于为什么会只有参数一个有值，在下文的源码分析中会一一解释。 源码分析现在只掌握了基本的消息发送，想要深刻的理解发送中的一些参数配置还是得源码说了算。 首先还是来谈谈消息发送时的整个流程是怎么样的，Kafka 并不是简单的把消息通过网络发送到了 broker 中，在 Java 内部还是经过了许多优化和设计。 发送流程为了直观的了解发送的流程，简单的画了几个在发送过程中关键的步骤。 从上至下依次是： 初始化以及真正发送消息的 kafka-producer-network-thread IO 线程。 将消息序列化。 得到需要发送的分区。 写入内部的一个缓存区中。 初始化的 IO 线程不断的消费这个缓存来发送消息。 步骤解析接下来详解每个步骤。 初始化 调用该构造方法进行初始化时，不止是简单的将基本参数写入 KafkaProducer。比较麻烦的是初始化 Sender 线程进行缓冲区消费。 初始化 IO 线程处： 可以看到 Sender 线程有需要成员变量，比如： 1acks,retries,requestTimeout 等，这些参数会在后文分析。 序列化消息在调用 send() 函数后其实第一步就是序列化，毕竟我们的消息需要通过网络才能发送到 Kafka。 其中的 valueSerializer.serialize(record.topic(), record.value()); 是一个接口，我们需要在初始化时候指定序列化实现类。 我们也可以自己实现序列化，只需要实现 org.apache.kafka.common.serialization.Serializer 接口即可。 路由分区接下来就是路由分区，通常我们使用的 Topic 为了实现扩展性以及高性能都会创建多个分区。 如果是一个分区好说，所有消息都往里面写入即可。 但多个分区就不可避免需要知道写入哪个分区。 通常有三种方式。 指定分区可以在构建 ProducerRecord 为每条消息指定分区。 这样在路由时会判断是否有指定，有就直接使用该分区。 这种一般在特殊场景下会使用。 自定义路由策略 如果没有指定分区，则会调用 partitioner.partition 接口执行自定义分区策略。 而我们也只需要自定义一个类实现 org.apache.kafka.clients.producer.Partitioner 接口，同时在创建 KafkaProducer 实例时配置 partitioner.class 参数。 通常需要自定义分区一般是在想尽量的保证消息的顺序性。 或者是写入某些特有的分区，由特别的消费者来进行处理等。 默认策略最后一种则是默认的路由策略，如果我们啥都没做就会执行该策略。 该策略也会使得消息分配的比较均匀。 来看看它的实现： 简单的来说分为以下几步： 获取 Topic 分区数。 将内部维护的一个线程安全计数器 +1。 与分区数取模得到分区编号。 其实这就是很典型的轮询算法，所以只要分区数不频繁变动这种方式也会比较均匀。 写入内部缓存在 send() 方法拿到分区后会调用一个 append() 函数： 该函数中会调用一个 getOrCreateDeque() 写入到一个内部缓存中 batches。 消费缓存在最开始初始化的 IO 线程其实是一个守护线程，它会一直消费这些数据。 通过图中的几个函数会获取到之前写入的数据。这块内容可以不必深究，但其中有个 completeBatch 方法却非常关键。 调用该方法时候肯定已经是消息发送完毕了，所以会调用 batch.done() 来完成之前我们在 send() 方法中定义的回调接口。 从这里也可以看出为什么之前说发送完成后元数据和异常信息只会出现一个。 Producer 参数解析发送流程讲完了再来看看 Producer 中比较重要的几个参数。 acksacks 是一个影响消息吞吐量的一个关键参数。 主要有 [all、-1, 0, 1] 这几个选项，默认为 1。 由于 Kafka 不是采取的主备模式，而是采用类似于 Zookeeper 的主备模式。 前提是 Topic 配置副本数量 replica &gt; 1。 当 acks = all/-1 时： 意味着会确保所有的 follower 副本都完成数据的写入才会返回。 这样可以保证消息不会丢失！ 但同时性能和吞吐量却是最低的。 当 acks = 0 时： producer 不会等待副本的任何响应，这样最容易丢失消息但同时性能却是最好的！ 当 acks = 1 时： 这是一种折中的方案，它会等待副本 Leader 响应，但不会等到 follower 的响应。 一旦 Leader 挂掉消息就会丢失。但性能和消息安全性都得到了一定的保证。 batch.size这个参数看名称就知道是内部缓存区的大小限制，对他适当的调大可以提高吞吐量。 但也不能极端，调太大会浪费内存。小了也发挥不了作用，也是一个典型的时间和空间的权衡。 上图是几个使用的体现。 retriesretries 该参数主要是来做重试使用，当发生一些网络抖动都会造成重试。 这个参数也就是限制重试次数。 但也有一些其他问题。 因为是重发所以消息顺序可能不会一致，这也是上文提到就算是一个分区消息也不会是完全顺序的情况。 还是由于网络问题，本来消息已经成功写入了但是没有成功响应给 producer，进行重试时就可能会出现消息重复。这种只能是消费者进行幂等处理。 高效的发送方式如果消息量真的非常大，同时又需要尽快的将消息发送到 Kafka。一个 producer 始终会收到缓存大小等影响。 那是否可以创建多个 producer 来进行发送呢？ 配置一个最大 producer 个数。 发送消息时首先获取一个 producer，获取的同时判断是否达到最大上限，没有就新建一个同时保存到内部的 List 中，保存时做好同步处理防止并发问题。 获取发送者时可以按照默认的分区策略使用轮询的方式获取（保证使用均匀）。 这样在大量、频繁的消息发送场景中可以提高发送效率减轻单个 producer 的压力。 关闭 Producer最后则是 Producer 的关闭，Producer 在使用过程中消耗了不少资源（线程、内存、网络等）因此需要显式的关闭从而回收这些资源。 默认的 close() 方法和带有超时时间的方法都是在一定的时间后强制关闭。 但在过期之前都会处理完剩余的任务。 所以使用哪一个得视情况而定。","categories":[{"name":"消息队列","slug":"消息队列","permalink":"https://damoshushu.github.io/categories/消息队列/"}],"tags":[{"name":"云服务","slug":"云服务","permalink":"https://damoshushu.github.io/tags/云服务/"},{"name":"消息队列","slug":"消息队列","permalink":"https://damoshushu.github.io/tags/消息队列/"}]},{"title":"常见消息队列比较","slug":"MessageQueueComparasion","date":"2018-11-15T23:00:06.000Z","updated":"2019-01-12T10:18:25.000Z","comments":true,"path":"2018/11/15/MessageQueueComparasion/","link":"","permalink":"https://damoshushu.github.io/2018/11/15/MessageQueueComparasion/","excerpt":"","text":"消息队列比较 ActiveMQ 是Apache出品的、采用Java语言编写的完全基于JMS1.1规范的面向消息的中间件，为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。不过由于历史原因包袱太重，目前市场份额没有后面三种消息中间件多，其最新架构被命名为Apollo，号称下一代ActiveMQ，有兴趣的同学可行了解。 RabbitMQ 是采用Erlang语言实现的AMQP协议的消息中间件，最初起源于金融系统，用于在分布式系统中存储转发消息。RabbitMQ发展到今天，被越来越多的人认可，这和它在可靠性、可用性、扩展性、功能丰富等方面的卓越表现是分不开的。 Kafka 起初是由LinkedIn公司采用Scala语言开发的一个分布式、多分区、多副本且基于zookeeper协调的分布式消息系统，现已捐献给Apache基金会。它是一种高吞吐量的分布式发布订阅消息系统，以可水平扩展和高吞吐率而被广泛使用。目前越来越多的开源分布式处理系统如Cloudera、Apache Storm、Spark、Flink等都支持与Kafka集成。 RocketMQ 是阿里开源的消息中间件，目前已经捐献个Apache基金会，它是由Java语言开发的，具备高吞吐量、高可用性、适合大规模分布式系统应用等特点，经历过双11的洗礼，实力不容小觑。 ZeroMQ 号称史上最快的消息队列，基于C语言开发。ZeroMQ是一个消息处理队列库，可在多线程、多内核和主机之间弹性伸缩，虽然大多数时候我们习惯将其归入消息队列家族之中，但是其和前面的几款有着本质的区别，ZeroMQ本身就不是一个消息队列服务器，更像是一组底层网络通讯库，对原有的Socket API上加上一层封装而已。 目前市面上的消息中间件还有很多，比如腾讯系的PhxQueue、CMQ、CKafka，又比如基于Go语言的NSQ，有时人们也把类似Redis的产品也看做消息中间件的一种，当然它们都很优秀，但是本文篇幅限制无法穷极所有，下面会针对性的挑选RabbitMQ和Kafka两款典型的消息中间件来做分析，力求站在一个公平公正的立场来阐述消息中间件选型中的各个要点。","categories":[{"name":"云服务","slug":"云服务","permalink":"https://damoshushu.github.io/categories/云服务/"}],"tags":[{"name":"云服务","slug":"云服务","permalink":"https://damoshushu.github.io/tags/云服务/"},{"name":"消息队列","slug":"消息队列","permalink":"https://damoshushu.github.io/tags/消息队列/"},{"name":"Java","slug":"Java","permalink":"https://damoshushu.github.io/tags/Java/"}]},{"title":"各种服务发现的比较","slug":"Hello-DamoShushu","date":"2018-11-14T15:30:06.000Z","updated":"2019-01-12T10:18:25.000Z","comments":true,"path":"2018/11/14/Hello-DamoShushu/","link":"","permalink":"https://damoshushu.github.io/2018/11/14/Hello-DamoShushu/","excerpt":"","text":"服务发现的比较 CAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性） Spring Cloud: 它主要提供的模块包括：服务发现（Eureka），断路器（Hystrix），智能路由（Zuul），客户端负载均衡（Ribbon）等 Feature Consul zookeeper etcd euerka 服务健康检查 服务状态，内存，硬盘等 (弱)长连接，keepalive 连接心跳 可配支持 多数据中心 支持 — — — kv存储服务 支持 支持 支持 — 一致性 raft paxos raft — cap cp cp cp ap 使用接口(多语言能力) 支持http和dns 客户端 http/grpc http（sidecar） watch支持 全量/支持long polling 支持 支持 long polling 支持 long polling/大部分增量 自身监控 metrics — metrics metrics 安全 acl /https acl https支持（弱） — spring cloud集成 已支持 已支持 已支持 已支持","categories":[{"name":"云服务","slug":"云服务","permalink":"https://damoshushu.github.io/categories/云服务/"}],"tags":[{"name":"服务发现","slug":"服务发现","permalink":"https://damoshushu.github.io/tags/服务发现/"},{"name":"云服务","slug":"云服务","permalink":"https://damoshushu.github.io/tags/云服务/"}]}]}